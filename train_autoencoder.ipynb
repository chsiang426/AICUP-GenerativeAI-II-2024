{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"Training_dataset/aug_train/aug_imgs\"\n",
    "train_label_dir = \"Training_dataset/aug_train/aug_gts\"\n",
    "val_img_dir = \"Training_dataset/validation/imgs\"\n",
    "val_label_dir = \"Training_dataset/validation/gts\"\n",
    "\n",
    "train_imgs = sorted([os.path.join(train_img_dir, f) for f in os.listdir(train_img_dir) if f.endswith(\".jpg\")])\n",
    "train_labels = sorted([os.path.join(train_label_dir, f) for f in os.listdir(train_label_dir) if f.endswith(\".png\")])\n",
    "\n",
    "val_imgs = sorted([os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir) if f.endswith(\".jpg\")])\n",
    "val_labels = sorted([os.path.join(val_label_dir, f) for f in os.listdir(val_label_dir) if f.endswith(\".png\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列印訓練與驗證資料集數量\n",
    "train_imgs_length = len(train_imgs)\n",
    "train_labels_length = len(train_labels)\n",
    "val_imgs_length = len(val_imgs)\n",
    "val_labels_length = len(val_labels)\n",
    "\n",
    "print(\"train_imgs 長度:\", train_imgs_length)\n",
    "print(\"train_labels 長度:\", train_labels_length)\n",
    "print(\"val_imgs 長度:\", val_imgs_length)\n",
    "print(\"val_labels 長度:\", val_labels_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset, DataLoader, and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset): # 用於 training set 和 validation set 的 Dataset\n",
    "    def __init__(self, images, labels, img_size, mode='train'):\n",
    "      assert mode in ['train', 'val'] # mode 必須是' train' 或 'val'\n",
    "      self.img_size = img_size\n",
    "      self.mode = mode\n",
    "      self.images = images\n",
    "      self.gts = labels\n",
    "      self.filter_files()\n",
    "\n",
    "      # image 預處理操作: 調整大小、轉換為 Tensor、標準化\n",
    "      self.img_transform = transforms.Compose([\n",
    "          transforms.Resize((self.img_size, self.img_size)),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "      # ground truth 預處理操作: 調整大小並轉換為 Tensor\n",
    "      self.gt_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      image = Image.open(self.images[index]).convert('RGB') # 打開圖像文件，轉換為RGB\n",
    "      gt = Image.open(self.gts[index]).convert('L') # 打開圖像文件，轉換為灰階影像\n",
    "\n",
    "      image = self.img_transform(image)\n",
    "      gt = self.gt_transform(gt)\n",
    "      return image, gt\n",
    "\n",
    "    def filter_files(self):\n",
    "      # 確保 image 與 ground truth 數量必須匹配\n",
    "      assert len(self.images) == len(self.gts)\n",
    "      images, gts = [], []\n",
    "      for img_path, gt_path in zip(self.images, self.gts):\n",
    "          img = Image.open(img_path)\n",
    "          gt = Image.open(gt_path)\n",
    "          if img.size == gt.size:\n",
    "              images.append(img_path)\n",
    "              gts.append(gt_path)\n",
    "      self.images, self.gts = images, gts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestlDataset(Dataset):  # Used for the testing set dataset\n",
    "    def __init__(self, images, img_size):\n",
    "        self.images = images\n",
    "        self.img_size = img_size\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.images[index]).convert('RGB') # 打開圖像文件，轉換為RGB\n",
    "        image = self.img_transform(image)\n",
    "        name = os.path.basename(self.images[index]).replace('.jpg', '.png')\n",
    "        return image, name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 DataLoader 來包裝 training set 和 validation set\n",
    "# - mode: 設置數據集的模式，設定為 \"train \"則會做資料擴增\n",
    "tr_datastet = GeneralDataset(images=train_imgs,\n",
    "                labels=train_labels,\n",
    "                img_size=256, mode=\"train\")\n",
    "val_datastet = GeneralDataset(images=val_imgs,\n",
    "                labels=val_labels,\n",
    "                img_size=256, mode=\"val\")\n",
    "\n",
    "# 建立 DataLoader 來加載 training set 和 validation set\n",
    "tr_loader = DataLoader(dataset = tr_datastet, batch_size=64, shuffle=True,\n",
    "                  num_workers=2, pin_memory=False)\n",
    "val_loader = DataLoader(dataset = val_datastet, batch_size=64, shuffle=False,\n",
    "                  num_workers=2, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 DataLoader 是否成功載入 image 和 ground truth\n",
    "data_iter = iter(tr_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "image, label = images[0], labels[0]\n",
    "\n",
    "image = image.numpy()\n",
    "label = label.numpy()\n",
    "image = np.transpose(image, (1, 2, 0))\n",
    "label = np.transpose(label, (1, 2, 0))\n",
    "\n",
    "if image.min() < 0 or image.max() > 1:\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "plt.imshow(label, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "bce_loss_module = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.autoencoderPatchGAN import VGG16Generator, ConditionalDiscriminator\n",
    "\n",
    "# define model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# autoencoder-PatchGAN\n",
    "autoencoder_generator = VGG16Generator().to(device)\n",
    "autoencoder_discriminator = ConditionalDiscriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "# autoencoder-PatchGAN\n",
    "optimizer_autoencoder_generator = optim.Adam(autoencoder_generator.parameters(), lr=0.0002, betas=(0.5, 0.99))\n",
    "optimizer_autoencoder_discriminator = optim.Adam(autoencoder_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training loop (autoencoder-PatchGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_models_path = 'models_autoencoder' # 模型權重儲存路徑\n",
    "os.makedirs(autoencoder_models_path, exist_ok=True)\n",
    "os.makedirs(\"autoencoder_validation_output\", exist_ok=True)\n",
    "\n",
    "epoch_num = 1000\n",
    "for epoch in range(epoch_num):\n",
    "    ## -------------Training stage--------------\n",
    "    autoencoder_generator.train()\n",
    "    autoencoder_discriminator.train()\n",
    "\n",
    "    loss_all_g = 0\n",
    "    loss_all_d = 0\n",
    "    epoch_step = 0\n",
    "    for images, gts in tr_loader:\n",
    "        images, gts = images.to(device), gts.to(device)\n",
    "\n",
    "        # update discriminator\n",
    "        real_d = autoencoder_discriminator(images, gts)\n",
    "        real_loss_d = bce_loss_module(real_d, torch.ones_like(real_d))\n",
    "        fake_images = autoencoder_generator(images).detach()\n",
    "        fake_d = autoencoder_discriminator(images, fake_images)\n",
    "        fake_loss_d = bce_loss_module(fake_d, torch.zeros_like(fake_d))\n",
    "        loss_d = 0.5 * (real_loss_d + fake_loss_d)\n",
    "\n",
    "        optimizer_autoencoder_discriminator.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optimizer_autoencoder_discriminator.step()\n",
    "        loss_all_d += loss_d.item()\n",
    "\n",
    "        # update generator every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            fake_images = autoencoder_generator(images)\n",
    "            fake_d = autoencoder_discriminator(images, fake_images)\n",
    "            fake_loss_g = bce_loss_module(fake_d, torch.ones_like(fake_d))\n",
    "            l1 = l1_loss(fake_images, gts) * 100\n",
    "            loss_g = fake_loss_g + l1\n",
    "\n",
    "            optimizer_autoencoder_generator.zero_grad()\n",
    "            loss_g.backward()\n",
    "            optimizer_autoencoder_generator.step()\n",
    "            loss_all_g += loss_g.item()\n",
    "\n",
    "        epoch_step += 1\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        avg_train_loss_g = loss_all_g/ epoch_step\n",
    "    avg_train_loss_d = loss_all_d/ epoch_step\n",
    "\n",
    "    # 每 5 個 epoch 儲存一次模型權重\n",
    "    if epoch % 5 == 0:\n",
    "        model_save_path = os.path.join(autoencoder_models_path, f'Net_epoch_{epoch}.pth')\n",
    "        torch.save(autoencoder_generator.state_dict(), model_save_path)\n",
    "        print(f'Model saved at epoch {epoch}')\n",
    "\n",
    "    # 每 5 個 epoch 保存一次生成的驗證圖像\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            for j, (val_img, val_label) in enumerate(val_loader):\n",
    "                val_img = val_img.to(device)\n",
    "                gen_val_label = autoencoder_generator(val_img)\n",
    "                save_image(gen_val_label.data, f\"autoencoder_validation_output/generated_{epoch}.png\")\n",
    "                break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epoch_num}], Generator Train Loss: {avg_train_loss_g:.4f}, Discriminator Train Loss: {avg_train_loss_d:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
